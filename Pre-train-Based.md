# Pre-train Based

[Advanced pre-training language models a brief introduction Slides](slides/presentation/Advanced%20pre-training%20language%20models%20a%20brief%20introduction.pdf)

## Survey

| Paper | Conference |
| :---: | :---: |
|[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)||

## Knowledge

| Paper | Conference |
| :---: | :---: |
|K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters||
|Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model |ICLR20|
|A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation|TACL19|


| Paper | Conference |
| :---: | :---: |
|Make Lead Bias in Your Favor : A Simple and Effective Method for News Summarization||
|DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation||
| What Does BERT Look At? An Analysis of BERTâ€™s Attention | EMNLP19 |
|SpanBERT: Improving Pre-training by Representing and Predicting Spans||
|XLNet: Generalized Autoregressive Pretraining for Language Understanding||
|Pre-Training with Whole Word Masking for Chinese BERT||
|Unified Language Model Pre-training for Natural Language Understanding and Generation||
|ERNIE: Enhanced Representation through Knowledge Integration||
|ERNIE: Enhanced Language Representation with Informative Entities|ACL19|
|MASS: Masked Sequence to Sequence Pre-training for Language Generation|ICML19|
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|NAACL19|
|Linguistic Knowledge and Transferability of Contextual Representations|NAACL19|
|Improving Language Understanding by Generative Pre-Training||
|Deep contextualized word representations|NAACL18|

